\section{Rasterization, More Complicated Than You Thought}

My apologies for potentially implying super-sampling and multi-sampling would be next, but I feel time needs to be spent with rasterization first.
It is necessary to understand rasterization to understand those two anti-aliasing methods, and like anti-aliasing I suspect it is something not many understand very well.
I certainly do not believe my understanding of it was accurate prior to this.

Before starting the research for this article I thought rasterization was the process of sampling the primitives in a game to get the pixel color.
While this is kind of true, it is also kind of wrong because there is more going on at this step, and it is relevant to both super-sampling and multi-sampling, as well as forward and deferred rendering.
You may have heard about forward and deferred rendering or shading before without really knowing what this meant, so I will try to explain them here, as it is not too much of a detour.

To understand rasterization I want you to visualize the world of any game.
Within that world a shape called a frustum is created, which for games is typically something like a pyramid with its top cut off; there are two similar rectangles parallel with each other and their vertices connected.
What is contained within the frustum is what will be shown by the camera, so everything outside of it can and often is culled or removed by the game engine or GPU itself.
This is why turning away from some effects or animations may improve performance in a game, because it is no longer being rendered.
From the front, smaller rectangle a ray is cast out to the back rectangle.
This ray will not bounce, which would be the defining characteristic of ray tracing, but it is cast and determined if it intersects a primitive.
If it does, it measures or samples the primitive, with the values for these samples derived from interpolating the values at the primitives vertices to every sample position.\supercite{DX10RasterizationStage, DX10RasterizationStart, DX10RasterizationRules}

Trying to translate that, information is not known within the primitive but at its vertices, so when a sample position is within the primitive, this vertex information is used to construct what the information should be within the primitive.
If one vertex is defined as being black and another as white, the interpolated color for a sample between them will be a grey, with the specific shade of grey depending on the position between the vertices.

%FRUSTUM
\image{Frustum}

If I wanted to keep it really simple I would stop here and say those samples are used to determine the color of the pixel you see then move on to super-sampling, but I want to be more accurate than that.
The information collected from this sampling is collected together as a fragment\supercite{OpenGLRas, OpenGLFragment}, which is one of the two definitions for fragment I have encountered.
The other refers to the portion of a primitive that is contained within the frustum, so a group of fragments following the previous definition.
These two definitions are related, but there can be cases when it is a single set of samples being referred to, instead of the larger group for a primitive or its portion contained in the frustum.

Something important here is that the measurements made when the ray intersects the object cover more than just color.
Information like color, material, normal, stencil, and Z are all measured and stored in the appropriate buffers, and all of this information can then be used to determine the final color of the pixel through a process called shading.
This is the work that shaders do on a GPU and additional effects can be applied by them, which influence the final color of the pixel.

It is in the shading process, or specifically a portion of it, that the differences between forward and deferred rendering, or shading exist.
With forward shading, all fragments are used for the lighting calculations while deferred shading waits to do the lighting until a pass identifies the non-occluded fragments.\supercite{ForwardDeferred, DeferredShadingUnity}
This means only as many lighting calculations are done as there are pixels in the final image with deferred, improving performance, but the process also hits memory bandwidth harder and risks mistakes due to a lack of information.
You see, by only using non-occluded fragments, if a blocked fragment should influence the lighting of another, like with a transparent texture, the information is not there for deferred shading.
Shadows calculations require the 'forward' information though, and so are not done using this deferred method.

The way the Z buffer works is it stores a value between 0 and 1 for the position of the fragment between the front and back of the frustum.
This would also be the depth of the fragment, which is why this is sometimes called the depth buffer.
If multiple fragments have the same X and Y positions, this Z value will determine which should be shown, which is very important because you may otherwise have the back of an object shown to you, instead of the front.
This next graphic is meant to illustrate rasterization of a simple group of three shapes.
The resulting fragments are shown as are the fragments after Z-culling, and while this example should have no difference between forward and deferred rendering, I hope you can imagine how it could have an impact.
I have some examples for later on that might do a better job demonstrating the difference, but these will be in reference to post-processing methods.

%Fragments
\image{Fragments}

Fortunately, it appears a number of solutions have been developed to overcome some of the potential disadvantages of deferred shading, including that multi-sampling could not be used with it, but there are still certain advantages to using forward shading instead.
Developers do need to make a decision from the beginning of which method to use.
(The solutions I have seen described for multi-sampling are to identify locations that have multiples samples, such as along the edge of the triangle, and identifying them as 'complex pixels' so they can be handled appropriately.)

Coming back to rasterization itself for a bit, there is a specific kind of aliasing artifact that can occur when rasterizing a scene called polygon popping.
The idea is very simple and is really what the name itself describes, with polygons popping in and out.

%POLYGON_POPPING
\image{Polygon Popping}

In this image I had the same reference image rendered out at the different resolutions labelled above each example, and then scaled them up to match.
(This was done with point sampling and it is integer scaling, so there are no distortions from this step.)
At the lowest resolution shown of 32x32, you can see a number of the primitives are very different from the reference, being in different positions, and even vanishing because of the positioning of the sampling rays.
If the objects of the same colors were in motion, from one position or orientation, to another, you would see details or the object themselves popping in and out, hence the name.

Another example I have uses the stacked shapes from earlier and I refer to it as color popping instead.
While what is happening is exactly polygon popping, the most obvious impact is on the colors.
This is especially true in the lower row as one example goes from just blue to just red and yellow (and white for empty).
Because the polygons popping in and out are stacked, the result is the colors popping in and out.

%COLOR_POPPING
\image{Color Popping - H}

It is also possible to demonstrate this with the sine waves from earlier, by applying an offset to the sampling positions of just 0.25, both positive and negative.

%ALIAS_EXAMPLE_TEMPORAL
\image{Alias Example - Temporal}

Despite the offset being a quarter the distance between samples, the gradients appear to shift by double the sampling distance.
I think this does a pretty good job of demonstrating popping but also a kind of temporal aliasing.
A shift of just 0.25, which you know by my statement but also by the color-matching rug plot at the bottom of the graph, results in the gradients shifting by 2.
If instead of the sampling position shifting by 0.25 it was the signal itself moving by 0.25 per unit time, the resulting gradient would appear to be moving eight times faster ($2 / 0.25$)!

Returning to spatial aliasing, which allows $sin(0.1x + 0.5)$ and $sin(0.9x + 0.5)$ to be confused, a way to avoid this aliasing is to increase the sampling frequency or resolution.
There is a frequency at which aliasing can be prevented, and thanks to Nyquist-Shannon sampling theorem we know this to be double the original signal's frequency.
There are some exceptions to this, with radio broadcasts being one, and the theorem refers to the ability to reconstruct the original signal, not just if it can be visually identified, which are both points I think are worth keeping in mind.

The frequency of the original function, $sin(0.9x + 0.5)$, is 0.9, so doubling the frequency gets us to 1.8.
Changing this to a length, we get 0.5556 ($1 / 1.8$) as the distance between samples, and as you can see in this graph, it definitely does a better job capturing details of the original function, compared to the sample distance of 1 used earlier.

%NYQUIST_RATE
\image{Alias Example - Nyquist - Rate}

The fact the amplitude of the samples does not match the original signal is not too great an issue.
It is just caused by starting at 0.5 as opposed to another position, like 0.25.
It would make the contrast between the sections of the gradients easier to see, but it is still the case we can see the alternating nature of the original signal.

Of course increasing the sampling rate even more will produce a still clearer image of the original signal, so I did this by doubling the Nyquist rate to 0.2778 ($1 / 3.6$), producing this graph.

%NYQUIST_RATE_x2
\image{Alias Example - Nyquist - Rate x2}

For an easier comparison between these sampling options, I have built another graph just of the gradients, but also added another that may be interesting to consider.
It is much wider than the graphs shown already, to help show off any long patterns within them.

%RATE_COMPARISON
\image{Alias Example - Rate Comparison}

At the top we see a gradient in red for the low-frequency signal that can be an alias for the high frequency signal, and below it the gradient where the distance between samples is 1.
We can see how well it matches the alias signal, even though it is made from the original signal shown at the bottom in blue.

The next gradient is the interesting one I decided to include, as it corresponds to a sampling width of 0.5, which is better than the Nyquist rate, yet it does not necessarily look better.
We can see the high-frequency pattern of alternating white and black sections, but there is also a low-frequency pattern where the sections are shades of grey.
This is not desirable in graphics because that low-frequency pattern could be noticed.

The next two gradients come from the Nyquist rate and then double this rate, before ending on the original signal.

While the 0.5-width gradient does complicate things with its low-frequency pattern, we do still see the general trend that increasing the sampling rate, decreasing the distance between samples helps to avoid aliasing.
This leads us to the first anti-aliasing method I want to discuss, which is super-sampling.